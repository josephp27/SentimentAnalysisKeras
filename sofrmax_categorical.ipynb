{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras to load the dataset with the top_words\n",
    "top_words = 8000\n",
    "(X_train, y_train), (_,_) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "# Pad the sequence to the same length\n",
    "max_review_length = 1085\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 1085, 50)          400000    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 460,602\n",
      "Trainable params: 460,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 165s 7ms/step - loss: 0.6617 - acc: 0.5978 - val_loss: 0.6444 - val_acc: 0.6324\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 164s 7ms/step - loss: 0.5524 - acc: 0.7527 - val_loss: 0.3959 - val_acc: 0.8396\n",
      "Epoch 3/5\n",
      "22500/22500 [==============================] - 164s 7ms/step - loss: 0.3040 - acc: 0.8766 - val_loss: 0.3137 - val_acc: 0.8752\n",
      "Epoch 4/5\n",
      "22500/22500 [==============================] - 164s 7ms/step - loss: 0.2122 - acc: 0.9183 - val_loss: 0.3014 - val_acc: 0.8804\n",
      "Epoch 5/5\n",
      "22500/22500 [==============================] - 164s 7ms/step - loss: 0.1666 - acc: 0.9400 - val_loss: 0.3045 - val_acc: 0.8848\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,batch_size=512 ,validation_split=0.1, epochs=5)\n",
    "\n",
    "model.save(\"trained_demo_embedding_still_50.h5\")\n",
    "#model.load_weights('trained_demo.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9494286  0.05057139]]\n"
     ]
    }
   ],
   "source": [
    "# words = 'i hate this movie'\n",
    "# test = np.array([word_index[word] if word in word_index else 0 for word in words])\n",
    "\n",
    "# test=sequence.pad_sequences([test],maxlen=max_review_length)\n",
    "# print(model.predict(test))\n",
    "\n",
    "#words = ' They ensure it’s a solitary position to leave her on an island, set strict occupational rules for her to break, and very deliberately lead their antagonist through a series of meticulously designed pylons of human shaped food for entertainment. It’s factory-line horror as predictable as it is bland.'\n",
    "#test = np.array([word_index[word] if word in word_index else 0 for word in words])\n",
    "\n",
    "words = '''i really like this movie'''\n",
    "test = np.array([word_index[word] if word in word_index else 0 for word in words])\n",
    "\n",
    "test = np.asarray([0 if num > 10000 else num for num in test])\n",
    "test=sequence.pad_sequences([test],maxlen=max_review_length)\n",
    "print(model.predict(test))\n",
    "\n",
    "# test=sequence.pad_sequences([test],maxlen=max_review_length)\n",
    "# print(model.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
